## Overview

Modern AI applications are evolving beyond single-model prompts. Agentic AI systems require:
- Reliable access to **external tools**, **files**, and **contextual data**
- Seamless **inter-agent communication** across platforms
- Scalable orchestration of **multi-agent workflows**

This is where **MCP** and **A2A** come in.

---

## üîå Model Context Protocol (MCP)

MCP is an open standard introduced by [Anthropic](https://www.anthropic.com/news/model-context-protocol) that acts like a USB-C port for language models. It standardizes how models interface with:
- Filesystems and cloud storage
- Functions, APIs, or local executables
- Structured prompts and memory resources

MCP supports two transport layers:
- **STDIO**: Low-latency, subprocess-based communication for local integrations
- **Streamable HTTP**: Network-accessible endpoints using HTTP POSTs and streaming responses

üîó [MCP Spec](https://modelcontextprotocol.io/specification/2025-03-26/basic)

---

## üß† Agent2Agent Protocol (A2A)

A2A is a protocol developed by [Google](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability) that enables AI agents to:
- Discover each other via public ‚ÄúAgent Cards‚Äù
- Advertise their capabilities, endpoints, and auth requirements
- Exchange messages and delegate tasks across different platforms

A2A solves the problem of vendor lock-in and fragile, one-off integrations between agents. It allows modular, scalable multi-agent systems to cooperate‚Äîacross clouds, devices, and organizations.

üîó [A2A GitHub Repo](https://github.com/google/A2A)

---

## üéØ Why It Matters for Agentic AI & RAG

Together, **MCP** and **A2A** offer a foundation for building sophisticated agentic systems capable of:
- **Retrieval-Augmented Generation (RAG)** with external memory and context
- **Tool use and chaining**, enabling dynamic decision-making
- **Multi-agent collaboration**, supporting workflows where agents handle specialized tasks

These protocols unlock the next generation of intelligent applications.

## Additional details

### MCP
The Model Context Protocol (MCP) supports two primary transport mechanisms: stdio and Streamable HTTP (formerly SSE), each suited to different deployment scenarios.

**STDIO transport**

In the stdio mode, the MCP server operates as a local subprocess, communicating with the client through standard input and output streams. This setup offers minimal latency and full-duplex messaging, making it ideal for local integrations such as command-line tools or applications running on the same machine. Messages are encoded using JSON-RPC and are delimited by newlines, ensuring efficient and straightforward communication. This transport is particularly beneficial when network overhead needs to be minimized and when both client and server reside on the same host .

**Streamable HTTP transport**

The Streamable HTTP transport replaces the earlier SSE approach and allows the MCP server to function as an independent process accessible over the network. Clients send JSON-RPC messages via HTTP POST requests, while the server can push responses and notifications back through a persistent HTTP connection. This method facilitates remote access, browser compatibility, and easier traversal through firewalls. It's well-suited for scenarios where the client and server are on different machines or when integrating with web-based applications .

**Choosing between STDIO and Streamable HTTP**

The choice between stdio and Streamable HTTP depends on the specific requirements of your application:

- STDIO: Best for local, high-performance use cases where low latency and simplicity are paramount. Ideal for command-line interace and single-host implementations

- Streamable HTTP: Ideal for remote or distributed applications requiring broader accessibility and compatibility with web technologies.

By selecting the appropriate transport mechanism, developers can optimize the performance and accessibility of their MCP implementations.